{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890ce6b7-0db3-4c79-98b6-6a45b80dc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random,pickle,csv,cv2,os,scipy,pickle,warnings,matplotlib\n",
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0cd9f84-ea1e-4bb8-a812-68c0b9c2d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e8e8f10-ffa8-4f5f-943f-21006cbe63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"features_train_1\", \"rb\") as f:\n",
    "    features = np.array(pickle.load(f))\n",
    "with open(\"labels_train_1\", \"rb\") as f:\n",
    "    labels = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498b4d7-061f-4caf-bede-efb65d131183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5353592c-093b-4722-bf85-e344faac21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('new_dir.zip', 'r') as zip:\n",
    "    zip.extractall('self_driving_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e748079-f5de-4fa5-8b65-b643e626d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(img):\n",
    "    resized_image = cv2.resize((cv2.cvtColor(img,cv2.COLOR_RGB2HSV))[:,:,2],(32,32))\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedfbfc4-fa0e-41e9-88d2-b4f9893f9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training(delta):\n",
    "    logs = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    with open(labels_file,'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            logs.append(line)\n",
    "        log_labels = logs.pop(0)\n",
    "        \n",
    "    for i in range(len(logs)):\n",
    "        for j in range(3):\n",
    "            img_path = logs[i][j]\n",
    "            img_path = features_directory +  (img_path.split('IMG')[1]).strip()\n",
    "            img = plt.imread(img_path)\n",
    "            features.append(image_preprocessing(img))\n",
    "            \n",
    "            if j == 0:\n",
    "                labels.append(float(logs[i][3]))\n",
    "            elif j == 1:\n",
    "                labels.append(float(logs[i][3]) + delta)\n",
    "            else:\n",
    "                labels.append(float(logs[i][3]) - delta)\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1278e0-e5a9-4942-a843-f56fbb067812",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_directory = 'self_driving_data'\n",
    "#features_directory = '../input/self driving car training data/data/'\n",
    "labels_file = 'driving_log.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e3c24a-cba0-4953-baed-75f77004b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.2\n",
    "features,labels = load_training(delta)\n",
    "\n",
    "features = np.array(features).astype('float32')\n",
    "labels = np.array(labels).astype('float32')\n",
    "\n",
    "with open('features_new1','wb') as f:\n",
    "    pickle.dump(features,f,protocol=4)\n",
    "with open('labels_new1','wb') as f:\n",
    "    pickle.dump(labels,f,protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "644c7391-3dd4-422e-9b55-78b81ced0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"features_new\", \"rb\") as f:\n",
    " #   features = np.array(pickle.load(f))\n",
    "with open(\"labels_new\", \"rb\") as f:\n",
    "    labels = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50dc4a00-2794-4742-a71f-11ebd913ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = []\n",
    "A = np.zeros(shape = (3,32,32))\n",
    "for i in range(len(features)):\n",
    "    A[0,:,:] = features[i]\n",
    "    A[1,:,:] = features[i]\n",
    "    A[2,:,:] = features[i]\n",
    "    new_features.append(A)\n",
    "\n",
    "    \n",
    "with open('features_3d','wb') as f:\n",
    "    pickle.dump(new_features,f,protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51a2229-385c-4ef8-be35-ee0a2cc69ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features_3d\", \"rb\") as f:\n",
    "    features = np.array(pickle.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf16cbed-654d-4842-9728-357dd1bb4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(features)\n",
    "labels = torch.FloatTensor(labels)\n",
    "labels = labels.unsqueeze(1)\n",
    "A = list(features)\n",
    "B = list(labels)\n",
    "C = zip(A,B)\n",
    "D = list(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b4b9c4a-c0cd-44bc-bc83-4c14fdda6d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b030cfae-ea7c-4d40-b5b1-9a6f24468da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features[5000])\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "968ab507-1516-4338-9890-0528f677c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable all_transforms for later use\n",
    "all_transforms = transforms.Compose([transforms.Resize((40,40)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                                          std=[0.2023, 0.1994, 0.2010])\n",
    "                                     ])\n",
    "# Create Training dataset\n",
    "#train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    " #                                            train = True,\n",
    "  #                                           transform = all_transforms,\n",
    "   #                                          download = False)\n",
    "\n",
    "# Create Testing dataset\n",
    "#test_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    " #                                           train = False,\n",
    "  #                                          transform = all_transforms,\n",
    "   #                                         download=True)\n",
    "\n",
    "# Instantiate loader objects to facilitate processing\n",
    "train_loader = torch.utils.data.DataLoader(dataset = D,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    " #                                          batch_size = batch_size,\n",
    "  #                                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "750a1d48-0593-4899-9f4f-8b8e548716d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24108"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a375a519-c1a8-463f-a31d-b41bd5273ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "891542b6-ac44-4f8b-a90a-9bc314305cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "\t#  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1600, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        #out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0ee01fd-6c37-4b99-9ea2-1fe67e8916bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fca6903-484d-485e-9b50-051e2ccf753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0507\n",
      "Epoch [2/20], Loss: 0.0388\n",
      "Epoch [3/20], Loss: 0.0433\n",
      "Epoch [4/20], Loss: 0.0362\n",
      "Epoch [5/20], Loss: 0.0554\n",
      "Epoch [6/20], Loss: 0.0258\n",
      "Epoch [7/20], Loss: 0.0502\n",
      "Epoch [8/20], Loss: 0.0524\n",
      "Epoch [9/20], Loss: 0.0336\n",
      "Epoch [10/20], Loss: 0.0434\n",
      "Epoch [11/20], Loss: 0.0487\n",
      "Epoch [12/20], Loss: 0.0588\n",
      "Epoch [13/20], Loss: 0.0525\n",
      "Epoch [14/20], Loss: 0.0402\n",
      "Epoch [15/20], Loss: 0.0446\n",
      "Epoch [16/20], Loss: 0.0573\n",
      "Epoch [17/20], Loss: 0.0551\n",
      "Epoch [18/20], Loss: 0.0514\n",
      "Epoch [19/20], Loss: 0.0329\n",
      "Epoch [20/20], Loss: 0.0350\n"
     ]
    }
   ],
   "source": [
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "\t#Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181d7b4e-2559-4db0-baf3-9c232cb83865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50000 train images: 69.6 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network on the {} train images: {} %'.format(50000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efdbcc7-662a-497f-b6ac-374f1e9f99b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
